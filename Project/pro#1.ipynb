{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from bllipparser import RerankingParser\n",
    "from nltk.tree import Tree\n",
    "from bllipparser import RerankingParser, tokenize\n",
    "rrp = RerankingParser.from_unified_model_dir('/home/sai/models/WSJ+Gigaword')\n",
    "rrp.load_reranker_model('/home/sai/models/WSJ+Gigaword/reranker/features.gz', '/home/sai/models/WSJ+Gigaword/reranker/weights.gz')\n",
    "#nbest_list =rrp.parse('This is simple.')\n",
    "#print(nbest_list[0].ptb_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "sd_tokens",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8b5f924fe7e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                             \u001b[0;32mfor\u001b[0m \u001b[0mte\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparse_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptb_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msd_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nsubj'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnpnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                     \u001b[0msub_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sai/anaconda2/lib/python2.7/site-packages/bllipparser/CharniakParser.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0m__setattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0m__swig_getmethods__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_CharniakParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputTree_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sai/anaconda2/lib/python2.7/site-packages/bllipparser/CharniakParser.pyc\u001b[0m in \u001b[0;36m_swig_getattr\u001b[0;34m(self, class_type, name)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__swig_getmethods__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: sd_tokens"
     ]
    }
   ],
   "source": [
    "\n",
    "grammar = r\"\"\"\n",
    "  NP:         \n",
    "      {<PRP>}\n",
    "      {<RB>?<DT>?<JJ>*<NN>+}  # Chunk sequences of DT, JJ, NN\n",
    "      {<DT>?<JJ>*<NN>+}\n",
    "      #{<CD><NNS>+}\n",
    "      {<RB>?<CD>?<NNS>+}\n",
    "      {<NP>?<CC>?<NP>}\n",
    "      \n",
    "      \n",
    "\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  PPing: {<PP><VPing>}\n",
    "\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "      {<VBD><VBN>}\n",
    "      {<VB>|<VBP>}\n",
    "      {<VBZ>}\n",
    "      {<VBD><PPing>}\n",
    "      \n",
    "  VPing: \n",
    "      {<VBG>}\n",
    "\n",
    "  CLAUSE: {<NP><VP>}         # Chunk NP, VP\n",
    "  \"\"\"\n",
    "\n",
    "def foo(s):\n",
    "    return \"%s\",s\n",
    "\n",
    "with open('lm_data.txt',\"w\") as fn:\n",
    "    with open ('si_data.txt', \"w\") as fp:\n",
    "        with open ('tr_data.txt', \"w\") as ft:\n",
    "            in_list=[]\n",
    "            cp = nltk.RegexpParser(grammar)\n",
    "            sent_count=0\n",
    "            row=0\n",
    "            for i in sents:\n",
    "                pp_node=[]\n",
    "                sent_count+=1\n",
    "                text = word_tokenize(i)\n",
    "                tag_sent= nltk.pos_tag(text)\n",
    "               # tag_sent=nltk.pos_tag(i)\n",
    "                #tag_sent=rrp.tag(i)\n",
    "                chunk=cp.parse(tag_sent)\n",
    "                tree = chunk\n",
    "                for node in tree:\n",
    "                    if type(node) is nltk.Tree:\n",
    "                        if node.label()=='NP':\n",
    "                            npnode=foo(node)\n",
    "                            a=str(npnode)\n",
    "                            # npnode= nbest_list[0].ptb_parse\n",
    "                            #npnode=str(nbest_list[0].ptb_parse)\n",
    "                            npnew=nltk.Tree.fromstring(a,read_leaf=lambda x: x.split(\"/\")[0])\n",
    "                            parse_sent=rrp.parse(i)\n",
    "                            sub_ind=0\n",
    "                            #b=parse_sent[0].ptb_parse\n",
    "                           # text = nltk.word_tokenize(b)\n",
    "                            #c=list(text)\n",
    "                             # subject indicator or rootNN indicator\n",
    "                            #t=tree(parse_sent[0].ptb_parse)\n",
    "                            \n",
    "                            \n",
    "                            for te in parse_sent[0].ptb_parse.sd_tokens():\n",
    "                                if te.deprel == 'nsubj' and te.form==npnew.leaves()[-1]:\n",
    "                                    sub_ind=1\n",
    "                                if te.deprel=='root' and te.form==npnew.leaves()[-1] and (te.cpos=='NN' or te.cpos=='NNS'):\n",
    "                                    sub_ind=1\n",
    "                            print>>ft,sent_count,\",\",\" \".join(npnew.leaves()),npnew.leaves()[-1],sub_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(s):\n",
    "    return \"%s\",s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sai/Downloads/setall.txt',\"r\") as fr:\n",
    "    sents=list(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'car'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzr.lemmatize('cars')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'go'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lmtzr.lemmatize(\"goes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cat running ran cactus cactus cactus community community'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"cats running ran cactus cactuses cacti community communities\"\n",
    "\" \".join([lmtzr.lemmatize(i) for i in sent.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat running ran cactus cactus cactus community community\n"
     ]
    }
   ],
   "source": [
    "sent = \"cats running ran cactus cactuses cacti community communities\"\n",
    "l=[]\n",
    "for i in sent.split():\n",
    "    a=lmtzr.lemmatize(i)\n",
    "    l.append(a)\n",
    "    \n",
    "\n",
    "x=\" \".join(l)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P:y:t:h:o:n\n"
     ]
    }
   ],
   "source": [
    "print(\":\".join(\"Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s+e+z\n"
     ]
    }
   ],
   "source": [
    "a=\"+\"\n",
    "b=a.join(\"sez\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
